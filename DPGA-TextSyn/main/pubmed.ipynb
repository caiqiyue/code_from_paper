{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy.linalg import sqrtm\n",
    "from numpy import iscomplexobj, trace, cov\n",
    "from utils.compute_mauve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_length():\n",
    "    with open(length_path, 'r') as f:\n",
    "        length_list = json.load(f)\n",
    "    f.close()\n",
    "    random_length = np.random.choice(length_list)\n",
    "    return random_length\n",
    "\n",
    "def random_sample_author():\n",
    "    with open(author_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        author_list = [line.strip() for line in lines]\n",
    "    f.close()\n",
    "    random_author = np.random.choice(author_list)\n",
    "    return random_author\n",
    "\n",
    "def save_listdata_to_json(data, path):\n",
    "    filedir = os.path.dirname(path)\n",
    "    if not os.path.exists(filedir):\n",
    "        os.makedirs(filedir)\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        file.close()\n",
    "    else:\n",
    "        with open(path, 'r+', encoding='utf-8') as file:\n",
    "            file_data = json.load(file)  \n",
    "            file_data.extend(data)\n",
    "            file.seek(0) \n",
    "            json.dump(file_data, file, ensure_ascii=False, indent=4)\n",
    "        file.close()\n",
    "\n",
    "def get_length(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def json2csv(json_file,csv_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['text'])\n",
    "        for item in data:\n",
    "            clean_item = item.replace('\\r', ' ').replace('\\n', ' ').replace('\\\"', '')\n",
    "            writer.writerow([clean_item]) \n",
    "\n",
    "def get_path(index, category, extension):\n",
    "    return result_path + 'epoch_' + str(index) + '_' + category + '.' + extension\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def transfer_blank(text,p):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    text_token = encoding.encode(text)\n",
    "    new_text_token = [encoding.encode('_')[0] if random.random() < p else x for x in text_token]\n",
    "    return encoding.decode(new_text_token)\n",
    "\n",
    "def aggregate_data(index,lst):\n",
    "    data_all = []\n",
    "    for item in lst:\n",
    "        data_all.extend(load_json_data(get_path(index, item, 'json')))\n",
    "    save_listdata_to_json(data_all, get_path(index+1,'all','json'))\n",
    "\n",
    "def cross_choice(data):\n",
    "    lst_text = np.random.choice(data, 2, replace=False).tolist()\n",
    "    return random.choice([lst_text, lst_text[::-1]])\n",
    "\n",
    "def mutate_choice(data):\n",
    "    return random.choice(data)\n",
    "\n",
    "def generate_choice(data):\n",
    "    return np.random.choice(data, min(3,len(data)), replace=False).tolist()\n",
    "\n",
    "def count_selected_indices(part_sizes, selected_indices):\n",
    "    count_selected = [0] * len(part_sizes)\n",
    "    start_index = 0\n",
    "    for i, size in enumerate(part_sizes):\n",
    "        end_index = start_index + size\n",
    "        count_selected[i] = sum(1 for index in selected_indices if start_index <= index < end_index)\n",
    "        start_index = end_index\n",
    "    return count_selected\n",
    "\n",
    "def calculate_fid(embeddings1, embeddings2):\n",
    "    mu1, sigma1 = embeddings1.mean(axis=0), cov(embeddings1, rowvar=False)\n",
    "    mu2, sigma2 = embeddings2.mean(axis=0), cov(embeddings2, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    print(\"FID: \", fid)\n",
    "    return fid\n",
    "\n",
    "def calculate_all_metrics(synthetic_embeddings, original_embeddings):\n",
    "    method_name = \"\"\n",
    "    p_feats = synthetic_embeddings  \n",
    "    q_feats = original_embeddings\n",
    "    result = compute_mauve(p_feats, q_feats)\n",
    "    print(\"MAUVE: \", result.mauve)\n",
    "\n",
    "\n",
    "def self_similarity(embeddings):\n",
    "    similarity_matrix = cosine_similarity(embeddings.unsqueeze(0), embeddings.unsqueeze(1), dim=2)\n",
    "    mask = torch.ones_like(similarity_matrix) - torch.eye(similarity_matrix.size(0), device=similarity_matrix.device)\n",
    "    masked_similarity_matrix = similarity_matrix * mask\n",
    "    max_value, flat_index = masked_similarity_matrix.view(-1).max(0)  \n",
    "    max_index = (flat_index // masked_similarity_matrix.size(1), flat_index % masked_similarity_matrix.size(1))\n",
    "    average_similarity = masked_similarity_matrix.sum() / mask.sum()\n",
    "    print(f\"Average similarity of the dataset: {average_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_population(syn_num,num_threads):\n",
    "    def get_initdata(syn_num):\n",
    "        ret_lst = []\n",
    "        for i in range(syn_num):\n",
    "            success = False\n",
    "            while not success:\n",
    "                try:\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=\"gpt-3.5-turbo\",\n",
    "                        temperature=1.2,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"xxxxxxxxxxxxx\"},\n",
    "                            {\"role\": \"user\", \"content\": \"xxxxxxxxxxxxx\"},\n",
    "                        ]\n",
    "                    )\n",
    "                    ret = completion.choices[0].message.content.replace(\"\\n\", \" \")\n",
    "                    if get_length(ret) < 80 or get_length(ret) > 1000:\n",
    "                        continue\n",
    "                    ret_lst.append(ret)\n",
    "                    success = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error on data generation for index {i+1}! Trying again...\")\n",
    "                    print(f\"Error: {e}\")\n",
    "        return ret_lst\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(get_initdata, int(syn_num/num_threads)) for i in range(num_threads)]\n",
    "        results = []\n",
    "        with tqdm(total=num_threads, desc=\"get_initial_population\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                results.extend(result)\n",
    "                pbar.update(1)\n",
    "    save_listdata_to_json(results, get_path(1,'all','json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_elite_choice(train_embeddings, embeddings_pt, num_choice=2000, sigma=None, threshold=0.73):\n",
    "    distances = torch.cdist(train_embeddings, embeddings_pt)\n",
    "\n",
    "    closest_indices = torch.argmin(distances, dim=1)\n",
    "\n",
    "    votes = torch.zeros(embeddings_pt.shape[0], device=embeddings_pt.device)\n",
    "\n",
    "    for idx in closest_indices:\n",
    "        votes[idx] += 1\n",
    "    if sigma is not None and sigma > 0:\n",
    "        print(\"Adding noise with sigma =\", sigma)\n",
    "        noise = torch.normal(0, sigma, size=votes.shape, device=votes.device)\n",
    "        votes += noise\n",
    "        votes = torch.clamp(votes, min=0)\n",
    "    else:\n",
    "        print(\"No noise added!\")\n",
    "    top_values, top_indices = torch.topk(votes, embeddings_pt.shape[0])\n",
    "\n",
    "    selected_indices = []\n",
    "    temp=0\n",
    "    current_threshold = threshold\n",
    "\n",
    "    while len(selected_indices) < num_choice:\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if len(selected_indices) >= num_choice:\n",
    "                break\n",
    "            if idx.item() in selected_indices:\n",
    "                continue\n",
    "            if selected_indices:\n",
    "                selected_embeddings = embeddings_pt[selected_indices]\n",
    "                similarity = F.cosine_similarity(embeddings_pt[idx].unsqueeze(0), selected_embeddings)\n",
    "                if torch.any(similarity >= current_threshold):\n",
    "                    continue\n",
    "\n",
    "            selected_indices.append(idx.item())\n",
    "\n",
    "        if len(selected_indices) < num_choice:\n",
    "            current_threshold = round(current_threshold + 0.01, 2)\n",
    "        temp = len(selected_indices)\n",
    "    return selected_indices[:num_choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def select_tensors_by_probability(train_embeddings, probability):\n",
    "    bernoulli_samples = torch.bernoulli(torch.full((train_embeddings.size(0),), probability))\n",
    "    selected_tensors = train_embeddings[selected_indices]\n",
    "    return selected_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate1(text):\n",
    "    length = get_length(text)\n",
    "    blank = transfer_blank(text,p)\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=1.2,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"xxx\"},\n",
    "                    {\"role\": \"user\", \"content\": \"xxx\"},\n",
    "                ]\n",
    "            )\n",
    "            ret = completion.choices[0].message.content.replace(\"\\n\", \" \")\n",
    "            if abs(length-get_length(ret)) / length > 0.8:\n",
    "                continue\n",
    "            success = True\n",
    "            return ret\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate2(text, p=0.5):\n",
    "    length = get_length(text)\n",
    "\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=1.2,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"xxx\"},\n",
    "                    {\"role\": \"user\", \"content\": \"xxx\"},\n",
    "                ]\n",
    "            )\n",
    "            ret = completion.choices[0].message.content.replace(\"\\n\", \" \")\n",
    "            if abs(length-get_length(ret)) / length > 0.8:\n",
    "                continue\n",
    "            success = True\n",
    "            return ret\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross(text):\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            length = get_length(text)\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=1.2,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"xxx\"},\n",
    "                    {\"role\": \"user\", \"content\": \"xxx\"},\n",
    "                ]\n",
    "            )\n",
    "            ret = completion.choices[0].message.content.replace(\"\\n\", \" \")\n",
    "            if abs(length-get_length(ret)) / length > 0.8:\n",
    "                continue\n",
    "            success = True\n",
    "            return ret\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            length = random_sample_length()\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=1.2,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"xxx\"},\n",
    "                    {\"role\": \"user\", \"content\": \"xxx\"},\n",
    "                ]\n",
    "            )\n",
    "            ret = completion.choices[0].message.content.replace(\"\\n\", \" \")\n",
    "            if get_length(ret) < 80 or get_length(ret) > 1000:\n",
    "                continue\n",
    "\n",
    "            success = True\n",
    "            return ret\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_num = 2000\n",
    "syn_num = 10000\n",
    "max_workers = 100\n",
    "T = 10\n",
    "sigma = 0 #eps=inf\n",
    "sample_p = 1\n",
    "filter_p = 0.73\n",
    "result_path = 'xxxxx'\n",
    "\n",
    "part_sizes = []\n",
    "train_embeddings = torch.load(train_emb_path)\n",
    "\n",
    "for i in range(1,T+1):\n",
    "    client = OpenAI(base_url=\"xxxxxx\",api_key=\"sk-xxxxxxx\")\n",
    "    if i == 1:\n",
    "        if not os.path.exists(get_path(i, 'all', 'json')):\n",
    "            get_initial_population(syn_num,max_workers)\n",
    "        else:\n",
    "            syn_now_num = len(load_json_data(get_path(i,'all','json')))\n",
    "            if syn_now_num < syn_num:\n",
    "                get_initial_population(syn_num-syn_now_num,syn_num-syn_now_num)\n",
    "    data_all = load_json_data(get_path(i,'all','json'))\n",
    "    if not os.path.exists(get_path(i, 'all_emb', 'pt')) or len(torch.load(get_path(i,'all_emb','pt')))!=len(data_all):\n",
    "        get_embeddings(i,'all')\n",
    "\n",
    "    embeddings_pt = torch.load(get_path(i,'all_emb','pt'))\n",
    "\n",
    "    if sample_p != 1:\n",
    "        selected_indices = filter_elite_choice(select_tensors_by_probability(train_embeddings, sample_p),embeddings_pt,elite_num,sigma,filter_p)\n",
    "    else:\n",
    "        selected_indices = filter_elite_choice(train_embeddings,embeddings_pt,elite_num,sigma,filter_p)\n",
    "    if part_sizes != []:\n",
    "        count_selected = count_selected_indices(part_sizes, selected_indices)\n",
    "        print(count_selected)\n",
    "\n",
    "    part_sizes = [len(selected_indices), syn_num-len(selected_indices)]\n",
    "    if not os.path.exists(get_path(i,'elite','json')):\n",
    "        save_listdata_to_json([data_all[indice] for indice in selected_indices],get_path(i,'elite','json'))\n",
    "\n",
    "    if not os.path.exists(get_path(i, 'elite_emb', 'pt')):\n",
    "        elite_embeddings_pt = embeddings_pt[selected_indices]\n",
    "        torch.save(elite_embeddings_pt,get_path(i,'elite_emb','pt'))\n",
    "\n",
    "    calculate_all_metrics(torch.load(get_path(i,'elite_emb','pt')).cpu().numpy(),torch.load(train_emb_path).cpu().numpy())\n",
    "    calculate_fid(torch.load(get_path(i,'elite_emb','pt')).cpu().numpy(),torch.load(train_emb_path).cpu().numpy())\n",
    "    if i == T: \n",
    "        break\n",
    "\n",
    "    results_mutate1 = []\n",
    "    results_mutate2 = []\n",
    "    results_cross = []\n",
    "    results_generate = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor_mutate:\n",
    "        futures_mutate = [executor_mutate.submit(mutate1, data_all[indice]) for indice in selected_indices]\n",
    "        with tqdm(total=elite_num, desc=\"mutate1\") as pbar:\n",
    "            for future_mutate in as_completed(futures_mutate):\n",
    "                result_mutate = future_mutate.result()\n",
    "                results_mutate1.append(result_mutate)\n",
    "                pbar.update(1)\n",
    "    save_listdata_to_json(results_mutate1, get_path(i,'mutate1','json'))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor_mutate:\n",
    "        futures_mutate = [executor_mutate.submit(mutate2, data_all[indice]) for indice in selected_indices]\n",
    "        with tqdm(total=elite_num, desc=\"mutate2\") as pbar:\n",
    "            for future_mutate in as_completed(futures_mutate):\n",
    "                result_mutate = future_mutate.result()\n",
    "                results_mutate2.append(result_mutate)\n",
    "                pbar.update(1)\n",
    "    save_listdata_to_json(results_mutate2, get_path(i,'mutate2','json'))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor_cross:\n",
    "        futures_cross = [executor_cross.submit(cross, data_all[indice]) for indice in selected_indices]\n",
    "        with tqdm(total=elite_num, desc=\"cross\") as pbar:\n",
    "            for future_cross in as_completed(futures_cross):\n",
    "                result_cross = future_cross.result()\n",
    "                results_cross.append(result_cross)\n",
    "                pbar.update(1)\n",
    "    save_listdata_to_json(results_cross, get_path(i,'cross','json'))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor_generate:\n",
    "        futures_generate = [executor_generate.submit(generate) for _ in range(elite_num)]\n",
    "        with tqdm(total=elite_num, desc=\"generate\") as pbar:\n",
    "            for future_generate in as_completed(futures_generate):\n",
    "                result_generate = future_generate.result()\n",
    "                results_generate.append(result_generate)\n",
    "                pbar.update(1)\n",
    "    save_listdata_to_json(results_generate, get_path(i,'generate','json'))\n",
    "    \n",
    "    aggregate_data(i,['elite','mutate1','mutate2','cross','generate']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    embeddings = torch.load(get_path(i+1,'all_emb','pt'))\n",
    "    indices = torch.randperm(embeddings.size(0))[:1000]\n",
    "    random_samples = embeddings[indices]\n",
    "    self_similarity(random_samples.to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    calculate_all_metrics(torch.load(get_path(i+1,'all_emb','pt'))[2000:].cpu().numpy(),torch.load(train_emb_path).cpu().numpy())\n",
    "    calculate_fid(torch.load(get_path(i+1,'all_emb','pt'))[2000:].cpu().numpy(),torch.load(train_emb_path).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
